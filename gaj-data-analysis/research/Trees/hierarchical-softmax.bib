@incollection{NIPS2008_3583,
title = {A Scalable Hierarchical Distributed Language Model},
author = {Mnih, Andriy and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1081--1088},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf}
}

@INPROCEEDINGS{Morin+Bengio-2005,
     author = {Morin, Frederic and Bengio, Yoshua},
     editor = {Cowell, Robert G. and Ghahramani, Zoubin},
      title = {Hierarchical Probabilistic Neural Network Language Model},
  booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
       year = {2005},
      pages = {246--252},
  publisher = {Society for Artificial Intelligence and Statistics},
        url = {http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf},
   abstract = {In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.},
topics={Language},cat={C},
}
