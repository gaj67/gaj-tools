\documentclass[a4paper]{article}
\usepackage{graphicx,subcaption}
\usepackage{amsmath,amsfonts}
\usepackage{qtree}
\usepackage{tikz}
\title{Notes on Sequence Modelling}
\author{G.A. Jarrad}
\usepackage{accents}
\newcommand{\rvec}[1]{\accentset{\leftarrow}{#1}}
\newcommand{\Bb}[1]{%
  \expandafter\def\csname#1#1\endcsname%
  {\ensuremath{\mathbb #1}}}
\Bb X\Bb S


\begin{document}
\maketitle
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\section{Random Sequence Processes}
\label{sec:random-processes}
Consider a random process $R$, graphically depicted in Figure~\ref{fig:R-process}, that generates arbitrary sequences of values
of the form $\vec{r}_n=(r_1,r_2,\ldots,r_n)$, where the length of any particular sequence is 
governed by a random variable $N$. Let $\vec{R}_N=(R_1,R_2,\ldots,R_N)$ denote
the corresponding sequence of random variables, where $R_t$ denotes the $t$-th discrete stage in the sequence.
\begin{figure}[hbt]
\centering
\begin{tikzpicture}[cnode/.style={draw,circle,minimum size=3em,inner sep=3pt}]
    \node[cnode] (1) at (0,0) {$R_1$};
    \node[cnode] (2) at (2, 0)  {$R_2$};
    \node (t) at (4, 0) {$\cdots$};
    \node[cnode] (n) at (6, 0)  {$R_N$};

    \draw[->] (1) edge (2) ;
    \draw[->] (2) edge (t) ;
    \draw[->] (t) edge (n) ;
\end{tikzpicture}
\caption{\em A random process $R$ for generating sequences of arbitrary length $N$. The arrows indicate transitions from one stage in the sequence to the next.}
\label{fig:R-process}
\end{figure}

We assume that each $R_t$ randomly takes some discrete or continuous value $r_t\in{\cal R}$,
and hence the probability (or probability density) of observing a particular
sequence $\vec{r}_n$ of length $n$ is given by
\begin{eqnarray}
   p(\vec{R}_N\!=\!\vec{r}_n) & = & p(N=n)\; p(\vec{R}_n\!=\!\vec{r}_n)\,,
\end{eqnarray}
where
\begin{eqnarray}
p(\vec{R}_n\!=\!\vec{r}_n) & = & p(R_1\!=\!r_1,\ldots,R_n\!=\!r_n)\,.
\end{eqnarray}
In practice, this definition presupposes that we know we have observed a {\em complete} sequence that was initiated
at stage 1 and terminated at stage $n$.
Suppose instead that the sequence $\vec{r}_n$ was observed one stage at a time. How do we know if the
underlying process has actually terminated, or will instead
continue to generate another observed value
$r_{n+1}$, leading to the extended sequence $\vec{r}_{n+1}$? 
Similarly, how do we know that the first observed value $r_1$ was not in fact
part of a longer, unobserved sequence of values $(\ldots,r_0,r_1,\ldots)$?

In order to handle such difficulties, we consider any arbitrary sequence $\vec{r}_n$ to be {\em incomplete},
and explicitly denote the corresponding, complete sequence as $\langle\vec{r}_n\rangle$.
Additionally, we introduce the notion of {\em partially complete} sequences, 
defining a {\em start sequence} to be a sequence that has a definite start but an indefinite end,
denoted by $\langle\vec{r}_n]$, and futher defining an {\em end sequence} to be a sequence
that has a definite end but an indefinite start, denoted by $[\vec{r}_n\rangle$.

Under this augmented notation, knowledge about the start of a sequence can be encapsulated in 
a random indicator variable $\iota_t$, which takes on the value 1 if $R_{t+1}$ is definitely the first stage in the
sequence, or the value 0 if it is not. Similarly, the random indicator variable $\tau_{t}$
takes on the value 1 if $R_{t-1}$ is definitely the last stage in the sequence, or the value 0 if it is not.
We assume that the random process $R$ only ever produces complete sequences described by $\langle\vec{R}_N\rangle$,
independently of the observation process, which might provide partial or complete sequences of values.
Notionally, the indicators $\iota_0$ and $\tau_{N+1}$ can be thought to correspond to pseudo-stages 0 and $N+1$, such that
the generated sequence $\langle\vec{R}_N\rangle$ is initiated at stage 0 and terminated at stage $N+1$.
This augmented random process is depicted in Figure~\ref{fig:random-process}. 
\begin{figure}[hbt]
\centering
\begin{tikzpicture}[cnode/.style={draw,circle,minimum size=3em,inner sep=3pt}, onode/.style={fill=black!50, draw,circle,minimum size=1em}]
    \node (0i) at (-1,0) [right] {$\stackrel{\iota_0=1}{}$};
    \node[onode] (0) at (0,0) {};
    \node[cnode] (1) at (2,0) {$R_1$};
    \node[onode] (1t) at (2,-2) {};
    \node[cnode] (2) at (4, 0)  {$R_2$};
    \node[onode] (2t) at (4,-2) {};
    \node[cnode] (3) at (6, 0)  {$R_3$};
    \node[onode] (3t) at (6,-2) {};
    \node (t) at (8, 0) {$\cdots$};
    \node[onode] (tt) at (8, -2) {};

    \draw[->] (0) edge node [pos=0.5, above] {$\stackrel{\tau_1=0}{}$}  (1) ;
    \draw[->] (0) edge node [pos=0.5, below left] {$\stackrel{\tau_1=1}{}$} (1t) ;
    \draw[->] (1) edge node [pos=0.5, above] {$\stackrel{\tau_2=0}{}$}  (2) ;
    \draw[->] (1) edge node [pos=0.5, below left] {$\stackrel{\tau_2=1}{}$} (2t) ;
    \draw[->] (2) edge node [pos=0.5, above] {$\stackrel{\tau_3=0}{}$}  (3) ;
    \draw[->] (2) edge node [pos=0.5, below left] {$\stackrel{\tau_3=1}{}$} (3t) ;
    \draw[->] (3) edge node [pos=0.5, above] {$\stackrel{\tau_4=0}{}$}  (t) ;
    \draw[->] (3) edge node [pos=0.5, below left] {$\stackrel{\tau_4=1}{}$} (tt) ;
\end{tikzpicture}
\caption{\em A random process for generating complete sequences of arbitrary length,
with explicit stages for sequence initiation and termination. Multiple arrows exiting from a node indicate
different possible (mutually exclusive) stage transition pathways.}
\label{fig:random-process}
\end{figure}

The probability of a given complete sequence $\langle\vec{r}_n\rangle$ is now defined as
\begin{eqnarray}
   p(\langle\vec{r}_n\rangle)
& = & p(\iota_0\!=\!1,\tau_1\!=\!0,R_1=r_1,\ldots,\tau_n\!=\!0,R_n=r_n,\tau_{n+1}\!=\!1)
\,,
\end{eqnarray}
such that 
\begin{eqnarray}
   p(N\!=\!n) & = &  p(\iota_0\!=\!1,\tau_1\!=\!0,\ldots,\tau_n\!=\!0,\tau_{n+1}\!=\!1)\,.
\end{eqnarray}
Likewise, the probability
of a given start sequence $\langle\vec{r}_n]$ is defined as
\begin{eqnarray}
p(\langle\vec{r}_n]) 
& = & p(\iota_0\!=\!1,\tau_1\!=\!0,R_1=r_1,\ldots,\tau_n\!=\!0,R_n=r_n)\,,
\end{eqnarray}
and the probability of the end sequence $[\vec{r}_n\rangle$ is
\begin{eqnarray}
p([\vec{r}_n\rangle)
& = & p(\tau_1\!=\!0,R_1=r_1,\ldots,\tau_n\!=\!0,R_n=r_n,\tau_{n+1}\!=\!1)\,.
\end{eqnarray}
In the special case where we know in advance that a start sequence definitely does not terminate
at stage $n+1$,  we may instead write
\begin{eqnarray}
p(\langle\vec{r}_n!)
& = & p(\iota_0\!=\!1,\tau_1\!=\!0,R_1=r_1,\ldots,\tau_n\!=\!0,R_n=r_n,\tau_{n+1}\!=\!0)\,.
\end{eqnarray}
Likewise, if an end sequence definitely does not initiate at stage 0, then
\begin{eqnarray}
p(!\vec{r}_n\rangle)
& = & p(\iota_0\!=\!0,\tau_1\!=\!0,R_1=r_1,\ldots,\tau_n\!=\!0,R_n=r_n,\tau_{n+1}\!=\!1)\,.
\end{eqnarray}
The four remaining types of sequences, namely $[\vec{r}_n]$, $!\vec{r}_n]$, $[\vec{r}_n!$ and $!\vec{r}_n!$ , can be similarly defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Sequence Processes}
\label{sec:markov-processes}
In Section~\ref{sec:random-processes} we defined a random process $R$ and the sequences it generates.
We now assume that the process is also {\em causal}, meaning that each stage of a sequence,
including the termination stage, depends only on the preceding stages.
This causal process, depicted in Figure~\ref{fig:causal-process}, is simply the random process from
Figure~\ref{fig:random-process} with additional, explicit dependencies (in the form of dashed arrows).
Hence, under the Markov assumption of conditional independence,
the causal sequence process leads to the fully-dependent, conditional model
\begin{eqnarray}
p(\langle\vec{r}_n\rangle) & = &
p(\iota_0=1)
\prod_{t=1}^{n}\left\{p(\tau_t\!=\!0\,|\,\iota_0\!=\!1,\vec{\tau}_{t-1}\!=\!\vec{0},\vec{R}_{t-1}\!=\!\vec{r}_{t-1})\right.
\nonumber\\&&
\hspace*{20mm}\left.\times p(R_t\!=\!r_t\,|\,\iota_0\!=\!1,\vec{\tau}_t\!=\!\vec{0},\vec{R}_{t-1}\!=\!\vec{r}_{t-1})\right\}
\nonumber\\&&
p(\tau_{n+1}\!=\!1\:|\;\iota_0\!=\!1,\vec{\tau}_n\!=\!\vec{0},\vec{R}_n\!=\!\vec{r}_n)
\,.
\label{eq:temporal-model}
\end{eqnarray}
The related models for partially complete or incomplete sequences can be similarly obtained
by suitably modifying the corresponding boundary conditions for $\iota_0$ and $\tau_{n+1}$
--- refer to Section~\ref{sec:random-processes}.
\begin{figure}[hbt]
\centering
\begin{tikzpicture}[cnode/.style={draw,circle,minimum size=3em,inner sep=3pt}, onode/.style={fill=black!50, draw,circle,minimum size=1em}]
    \node (0i) at (-1,0) [right] {$\stackrel{\iota_0=1}{}$};
    \node[onode] (0) at (0,0) {};
    \node[cnode] (1) at (2,0) {$R_1$};
    \node[onode] (1t) at (2,-2) {};
    \node[cnode] (2) at (4, 0)  {$R_2$};
    \node[onode] (2t) at (4,-2) {};
    \node[cnode] (3) at (6, 0)  {$R_3$};
    \node[onode] (3t) at (6,-2) {};
    \node (t) at (8, 0) {$\cdots$};
    \node[onode] (tt) at (8, -2) {};

    \draw[->] (0) edge node [pos=0.5, above] {$\stackrel{\tau_1=0}{}$}  (1) ;
    \draw[->] (0) edge node [pos=0.5, below left] {$\stackrel{\tau_1=1}{}$} (1t) ;
    \draw[->] (1) edge node [pos=0.5, above] {$\stackrel{\tau_2=0}{}$}  (2) ;
    \draw[->] (1) edge node [pos=0.5, below left] {$\stackrel{\tau_2=1}{}$} (2t) ;
    \draw[->] (2) edge node [pos=0.5, above] {$\stackrel{\tau_3=0}{}$}  (3) ;
    \draw[->] (2) edge node [pos=0.5, below left] {$\stackrel{\tau_3=1}{}$} (3t) ;
    \draw[->] (3) edge node [pos=0.5, above] {$\stackrel{\tau_4=0}{}$}  (t) ;
    \draw[->] (3) edge node [pos=0.5, below left] {$\stackrel{\tau_4=1}{}$} (tt) ;

   \begin{scope}[dashed]
    \draw[->] (0) to[out=-30,in=-150] (2) ;
    \draw[->] (0) to[out=-40,in=-150] (3) ;
    \draw[->] (1) to[out=40,in=140] (3) ;
   \end{scope}
\end{tikzpicture}
\caption{\em A fully-dependent, causal process for generating complete sequences of arbitrary length.
Solid arrows indicate possible stage transitions.
Both dashed arrows and solid arrows indicate parent--child dependencies, such that the child node is conditionally dependent on the parent and all other previous nodes.}
\label{fig:causal-process}
\end{figure}


In practice, the causal model is usually simplified further by 
dropping some of the explicit (dashed) dependencies.
For example, one might limit the 
conditionality on past values to a maximum number $m$ of depenencies.
This leads to the so-called {\em $m$-th order Markov model}.
An example from the realm of natural language understanding is the lexicographical analysis of the character
sequences of words using bigrams (pairs of adjacent characters, corresponding to $m=1$), and trigrams 
(triples of adjacent characters, corresponding to $m=2$), et cetera.

In the special case of $m=1$, the first-order Markov model takes on the restricted conditional form
\begin{eqnarray}
p(\langle\vec{r}_n\rangle) & = &
p(\iota_0=1)\,p(\tau_1\!=\!0\,|\,\iota_0\!=\!1)
p(R_1\!=\!r_1\,|\,\iota_0\!=\!1,\tau_1\!=\!0)
\nonumber\\&&
\prod_{t=2}^{n}\left\{p(\tau_t\!=\!0\,|\,R_{t-1}\!=\!r_{t-1})\right.
\nonumber\\&&
\hspace*{10mm}\left.\times p(R_t\!=\!r_t\,|\,\tau_t\!=\!0,R_{t-1}\!=\!r_{t-1})\right\}
\nonumber\\&&
p(\tau_{n+1}\!=\!1\:|\;R_n\!=\!r_n)
\,.
\end{eqnarray}
This is just the strict Markov interpretation of the random process depicted in Figure~\ref{fig:random-process},
where each stage directly depends only on the previous stage {\em and} on the transition path between the two adjacent stages.

\section{Stateful Markov Sequence Processes}
Consider the first-order Markov process $R$ depicted in Figure~\ref{fig:random-process}.
Suppose now that the random variable $R_t$ at stage $t$ can be decomposed into the tuple
$R_t=(S_t,X_t)$, where $S_t$ is a discrete random variable taking values $s_t\in{\cal S}$, and $X_t$
is a discrete or continuous random variable taking values $x_t\in{\cal X}$.
We may call $S_t$ the {\em state} of the process at stage $t$, and $X_t$ its {\em value}.
Hence, as is usual, we may suppose that $X_t$ depends upon $S_t$, such that the stage transitions are between states.
Keeping to the first-order Markov interpretation of stage-to-stage dependencies leads to
the {\em stateful} process depicted in Figure~\ref{fig:stateful-process}, with full cross-dependencies between $(S_t,X_t)$ and
$(S_{t+1},X_{t+1})$.
\begin{figure}[hbt]
\centering
\begin{tikzpicture}[cnode/.style={draw,circle,minimum size=3em,inner sep=3pt}, onode/.style={fill=black!50, draw,circle,minimum size=1em}]
    \node (0i) at (-1,0) [right] {$\stackrel{\iota_0=1}{}$};
    \node[onode] (0) at (0,0) {};
    \node[cnode] (1) at (2,0) {$S_1$};
    \node[cnode] (1x) at (2,-2) {$X_1$};
    \node[onode] (1t) at (2,2) {};
    \node[cnode] (2) at (4, 0)  {$S_2$};
    \node[cnode] (2x) at (4, -2)  {$X_2$};
    \node[onode] (2t) at (4,2) {};
    \node[cnode] (3) at (6, 0)  {$S_3$};
    \node[cnode] (3x) at (6, -2)  {$X_3$};
    \node[onode] (3t) at (6,2) {};
    \node (t) at (8, 0) {$\cdots$};
    \node (tx) at (8, -2)  {};
    \node (tt) at (8,2) {};

    \draw[->] (0) edge node [pos=0.5, above] {$\stackrel{\tau_1=0}{}$}  (1) ;
    \draw[->] (0) edge node [pos=0.5, above left] {$\stackrel{\tau_1=1}{}$} (1t) ;
    \draw[->] (1) edge node [pos=0.5, above] {$\stackrel{\tau_2=0}{}$}  (2) ;
    \draw[->] (1) edge node [pos=0.4, above left] {$\stackrel{\tau_2=1}{}$} (2t) ;
    \draw[->] (2) edge node [pos=0.5, above] {$\stackrel{\tau_3=0}{}$}  (3) ;
    \draw[->] (2) edge node [pos=0.4, above left] {$\stackrel{\tau_3=1}{}$} (3t) ;
    \draw[->] (3) edge (t) ;
    \draw[->] (3) edge (tt) ;

   \begin{scope}[dashed]
    \draw[->] (1) to (1x) ;
    \draw[->] (1) to (2x) ;
    \draw[->] (1x) to (2) ;
    \draw[->] (1x) to (2x) ;
    \draw[->] (2) to (2x) ;
    \draw[->] (2) to (3x) ;
    \draw[->] (2x) to (3) ;
    \draw[->] (2x) to (3x) ;
    \draw[->] (3) to (3x) ;
    \draw[->] (3) to (tx) ;
    \draw[->] (3x) to (tx) ;
    \draw[->] (3x) to (t) ;
   \end{scope}
\end{tikzpicture}
\caption{\em A random process for generating complete, stateful  sequences of arbitrary length,
with explicit cross-dependencies between the states and values of adjacent stages.}
\label{fig:stateful-process}
\end{figure}

Hence, the fully-structured stateful model is now given by
\begin{eqnarray}
p(\iota_0,\vec{S}_n,\vec{X}_n,\vec{\tau}_{n+1}) & = & 
p(\iota_0)\,p(\tau_1\,|\,\iota_0)\,p(S_1\,|\,\iota_0,\tau_1)\,p(X_1\,|\,S_1)
\nonumber\\&&
\prod_{t=2}^{n}\left\{p(\tau_t\,|\,S_{t-1})\,p(S_t\,|\,\tau_t,S_{t-1},X_{t-1})\right.
\nonumber\\&&
\left.\hspace*{10mm}\times\, p(X_t\,|\,S_t,S_{t-1},X_{t-1})\right\}
\nonumber\\&&
p(\tau_{n+1}\,|\,S_n)
\,.
\end{eqnarray}
It is more usual, however, to further restrict the complexity of the stateful process
by also imposing the first-order Markov assumption at the level
of the state--value dependencies themselves. In terms of the process depicted in Figure~\ref{fig:stateful-process},
this means retaining only direct node-to-node dependencies (rather than stage-to-stage dependencies).
Hence, this restricted process, depicted in Figure~\ref{fig:stateful-1-process},
corresponds to the sequence model
\begin{eqnarray}
p(\iota_0,\vec{S}_n,\vec{X}_n,\vec{\tau}_{n+1}) & = & 
p(\iota_0)\,p(\tau_1\,|\,\iota_0)\,p(S_1\,|\,\iota_0)\,p(X_1\,|\,S_1)
\nonumber\\&&
\prod_{t=2}^{n}\left\{p(\tau_t\,|\,S_{t-1})\,p(S_t\,|\,S_{t-1})\,p(X_t\,|\,S_t)\right\}
\nonumber\\&&
p(\tau_{n+1}\,|\,S_n)
\,.
\end{eqnarray}
\begin{figure}[hbt]
\centering
\begin{tikzpicture}[cnode/.style={draw,circle,minimum size=3em,inner sep=3pt}, onode/.style={fill=black!50, draw,circle,minimum size=1em}]
    \node (0i) at (-1,0) [right] {$\stackrel{\iota_0=1}{}$};
    \node[onode] (0) at (0,0) {};
    \node[cnode] (1) at (2,0) {$S_1$};
    \node[cnode] (1x) at (2,-2) {$X_1$};
    \node[onode] (1t) at (2,2) {};
    \node[cnode] (2) at (4, 0)  {$S_2$};
    \node[cnode] (2x) at (4, -2)  {$X_2$};
    \node[onode] (2t) at (4,2) {};
    \node[cnode] (3) at (6, 0)  {$S_3$};
    \node[cnode] (3x) at (6, -2)  {$X_3$};
    \node[onode] (3t) at (6,2) {};
    \node (t) at (8, 0) {$\cdots$};
    \node (tx) at (8, -2)  {};
    \node (tt) at (8,2) {};

    \draw[->] (0) edge node [pos=0.5, above] {$\stackrel{\tau_1=0}{}$}  (1) ;
    \draw[->] (0) edge node [pos=0.5, above left] {$\stackrel{\tau_1=1}{}$} (1t) ;
    \draw[->] (1) edge node [pos=0.5, above] {$\stackrel{\tau_2=0}{}$}  (2) ;
    \draw[->] (1) edge node [pos=0.4, above left] {$\stackrel{\tau_2=1}{}$} (2t) ;
    \draw[->] (2) edge node [pos=0.5, above] {$\stackrel{\tau_3=0}{}$}  (3) ;
    \draw[->] (2) edge node [pos=0.4, above left] {$\stackrel{\tau_3=1}{}$} (3t) ;
    \draw[->] (3) edge (t) ;
    \draw[->] (3) edge (tt) ;

   \begin{scope}[dashed]
    \draw[->] (1) to (1x) ;
    \draw[->] (2) to (2x) ;
    \draw[->] (3) to (3x) ;
   \end{scope}
\end{tikzpicture}
\caption{\em A first-order Markov process for generating complete, stateful  sequences of arbitrary length.}
\label{fig:stateful-1-process}
\end{figure}

\section{Hidden-state Markov Sequence Processes}

Consider the stateful, first-order Markov process depicted by Figure~\ref{fig:1-stateful-1-process}.
Suppose now that the value of state $S_t$ at any stage $t$ is never observed, only the value of $X_t$.
Then the model~\eqref{eq:1-stateful-markov-model} may be considered to be a {\em hidden-state} Markov model (or HMM).
As such, the state of $S_t$ must be deduced from knowledge of the observed sequence $\vec{x}_n$. This is accomplished
via the forward--backward algortithm. The forward step commences with stages $0$ and $1$, by defining
\begin{eqnarray}
  \alpha_1(s_1) & = & p(\iota_0,X_1=x_1,S_1=s_1)
\nonumber\\
& = & p(\iota_0)\,p(S_1=s_1\,|\,\iota_0)\,p(X_1=x_1\,|\,S_1=s_1)
\nonumber\\
& = & p(\iota_0)\,p(s_1\,|\,\iota_0)\,p(x_1\,|\,s_1)
\,,
\end{eqnarray}
from equation~\eqref{eq:1-stateful-markov-model}, where the explicit variables $S_t$ and $X_t$ may be
dropped for convenience when the context is unambiguous.
Then it follows that
\begin{eqnarray}
  \alpha_2(s_2) & = & p(\iota_0,X_1=x_1,X_2=x_2,S_2=s_2)
\nonumber\\
& = & \sum_{s_1\in{\cal S}}p(\iota_0,x_1,s_1)\,p(s_2\,|\,s_1)\,p(x_2\,|\,s_2)
\nonumber\\
& = & \sum_{s_1\in{\cal S}}\alpha_1(s_1)\,p(s_2\,|\,s_1)\,p(x_2\,|\,s_2)\,,
\end{eqnarray}
and in general that
\begin{eqnarray}
   \alpha_t(s_t) & = & p(\iota_0,\vec{X}_t=\vec{x}_t,S_t=s_t)
\nonumber\\
& = & \left\{\sum_{s_{t-1}\in{\cal S}}\alpha_{t-1}(s_{t-1})\,p(s_t\,|\,s_{t-1})\right\}\,p(x_t\,|\,s_t)\,,
\end{eqnarray}
for $t=2,3,\ldots,n$.
Consequently, we may predict $S_t$ from a partially observed sequence $\vec{x}_t$ via
\begin{eqnarray}
  p(S_t=s_t\,|\,\iota_0,\vec{X}_t=\vec{x}_t) & = & \frac{p(\iota_0,\vec{X}_t=\vec{x}_t,S_t=s_t)}
     {p(\iota_0,\vec{X}_t=\vec{x}_t)}
~=~\frac{\alpha_t(s_t)}{\sum_{s_t'\in{\cal S}}\alpha_t(s_t')}\,.
\end{eqnarray}
Similarly, we may predict the next observation $X_{t+1}$ via
\begin{eqnarray}
  p(X_{t+1}=x_{t+1}\,|\,\iota_0,\vec{X}_t=\vec{x}_t) 
& = &
  \frac{p(\iota_0,\vec{X}_t=\vec{x}_t,X_{t+1}=x_{t+1})}{p(\iota_0,\vec{X}_t=\vec{x}_t)}
\nonumber\\
& = & 
 \frac{\sum_{s_{t+1}\in{\cal S}}\sum_{s_{t}\in{\cal S}}p(\iota_0,\vec{x}_t,s_t)\,p(s_{t+1}\,|\,s_t)\,p(x_{t+1}\,|\,s_{t+1})}
        {p(\iota_0,\vec{x}_t)}
\nonumber\\
& = &
  \frac{\sum_{s_{t+1}\in{\cal S}}\sum_{s_{t}\in{\cal S}}\alpha_t(s_t)\,p(s_{t+1}\,|\,s_t)\,p(x_{t+1}\,|\,s_{t+1})}{\sum_{s_t\in{\cal S}}\alpha_t(s_t)}\,.
\end{eqnarray}

The backward step now commences with stage $n+1$, by defining
\begin{eqnarray}
  \beta_{n}(s_{n}) & = & p(\tau_{n+1}\,|\,S_n=s_{n})
\,,
\end{eqnarray}
and 
\begin{eqnarray}
  \beta_{n-1}(s_{n-1}) & = & p(X_n=x_{n},\tau_{n+1}\,|\,S_{n-1}=s_{n-1})
\nonumber\\
& = &
\sum_{s_n\in{\cal S}}p(\tau_{n+1}\,|\,s_{n})\,p(x_n\,|\,s_n)\,p(s_n\,|\,s_{n-1})
\nonumber\\
& = &
\sum_{s_n\in{\cal S}}\beta_n(s_{n})\,p(x_n\,|\,s_n)\,p(s_n\,|\,s_{n-1})
\,.
\end{eqnarray}
In general, we let $\rvec{x}_t=(x_t,x_{t+1},\ldots,x_n)$, and then recursively define
\begin{eqnarray}
  \beta_{t}(s_{t}) & = & p(\rvec{X}_{t+1}=\rvec{x}_{t+1},\tau_{n+1}\,|\,S_t=s_{t})
\nonumber\\
& = &
\sum_{s_{t+1}\in{\cal S}}
  p(\rvec{x}_{t+2},\tau_{n+1}\,|\,s_{t+1})\,p(x_{t+1}\,|\,s_{t+1})\,p(s_{t+1}\,|\,s_t)
\nonumber\\
& = &
\sum_{s_{t+1}\in{\cal S}}\beta_{t+1}(s_{t+1})\,p(x_{t+1}\,|\,s_{t+1})\,p(s_{t+1}\,|\,s_t)
\,.
\end{eqnarray}
Consequently, for an observed sequence $\vec{x}_n$,  the forward--backward algorithm 
gives the stage $t$ probability
\begin{eqnarray}
  p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,S_t\!=\!s_t,\tau_{n+1}) & = & 
  p(\iota_0,\vec{X}_t\!=\!\vec{x}_t,S_t\!=\!s_t)
\nonumber\\&& 
\;\;p(\rvec{X}_{t+1}=\rvec{x}_{t+1},\tau_{n+1}\,|\,S_t\!=\!s_t)
\nonumber\\& = &
  \alpha_t(s_t)\beta_t(s_t)\,.
\end{eqnarray}
Thus, the probability of $\vec{x}_n$  is
\begin{eqnarray}
  p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1}) & = & 
  \sum_{s_t\in{\cal S}}p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,S_t\!=\!s_t,\tau_{n+1})
\nonumber\\& = &
  \sum_{s_t\in{\cal S}}\alpha_t(s_t)\beta_t(s_t)\,,
\end{eqnarray}
and the posterior prediction of the state $S_t$ given $\vec{x}_n$ is subsequently given by
\begin{eqnarray}
\gamma_t(s_t) & = &
  p(S_t\!=\!s_t\,|\,\iota_0,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1}) 
\nonumber\\& = & 
\frac{p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,S_t\!=\!s_t,\tau_{n+1})}
       {p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1})}
\nonumber\\& = &
\frac{\alpha_t(s_t)\beta_t(s_t)}{\sum_{s_t'\in{\cal S}}\alpha_t(s_t')\beta_t(s_t')}\,.
\label{eq:gamma-def}
\end{eqnarray}
Likewise, the posterior prediction of the state transition from stage $t$ to stage $t+1$ is given by
\begin{eqnarray}
\xi_t(s_t,s_{t+1}) & = &
   p(S_t\!=\!s_t,S_{t+1}\!=\!s_{t+1}\,|\,\iota_0,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1})
\nonumber\\& = & 
\frac{p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,S_t\!=\!s_t,S_{t+1}\!=\!s_{t+1},\tau_{n+1})}
        {p(\iota_0,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1})}
\nonumber\\& = & 
\frac{p(\iota_0,\vec{x}_t,s_t)\,p(s_{t+1}\,|\,s_t)\,p(x_{t+1}\,|\,s_{t+1})\,p(\rvec{x}_{t+2},\tau_{n+1}\,|\,s_{t+1})}
        {p(\iota_0,\vec{x}_n,\tau_{n+1})}
\nonumber\\& = & 
\frac{\alpha_t(s_t)\,p(s_{t+1}\,|\,s_t)\,p(x_{t+1}\,|\,s_{t+1})\,\beta_{t+1}(s_{t+1})}
        {\sum_{s_t'\in{\cal S}}\alpha_t(s_t')\beta_t(s_t')}
\,.
\label{eq:xi-def}
\end{eqnarray}

\section{Hidden-state Parameter Estimation}

Suppose now that the hidden-state Markov model~\eqref{eq:1-stateful-markov-model} implicitly depends upon some
parameter $\theta$, the value of which needs to be estimated from observed data.
In particular, let us assume that $\theta=(\Pi,\Gamma,\Omega)$,
where $\Pi=(\vec{\pi}^+,\vec{\pi}^-)$ 
and $\Omega=(\vec{\omega}^+,\vec{\omega}^-)$ respectively
specify the possible distributions of the initial and final
states of an arbitrary sequence (defined in further detail below),
and $\Gamma$ represents the {\em stationary} distribution of state transitions between stages.

For convenience, we now suppose that the discrete set of possible states is given by
${\cal S}=\{\sigma_1,\sigma_2,\ldots,\sigma_S\}$.
Then we may define the initial distributions of states via $\vec{\pi}=(\pi_{1},\pi_{2},\ldots,\pi_{S})$ where
\begin{eqnarray}
  \pi_{i}^+ & = & p(\iota_0\!=\!1\,|\,\theta)\,p(S_1\!=\!\sigma_i\,|\,\iota_0\!=\!1,\theta)\,,
\\
  \pi_{i}^- & = & p(\iota_0\!=\!0\,|\,\theta)\,p(S_1\!=\!\sigma_i\,|\,\iota_0\!=\!0,\theta)\,,
\end{eqnarray}
such that $\pi_{i}^*=\pi_i^{+}+\pi_i^{-}=p(S_1\!=\!\sigma_i\,|\,\theta)$.
Similarly, the final distributions of states are defined by $\vec{\omega}=(\omega_1,\ldots,\omega_S)$ where
\begin{eqnarray}
  \omega_{i}^+ & = & p(\tau_{n+1}=1\,|\,S_n\!=\!\sigma_i,\theta)\,,
\\
  \omega_{i}^- & = & p(\tau_{n+1}=0\,|\,S_n\!=\!\sigma_i,\theta)\,,
\end{eqnarray}
such that $\omega_{i}^*=\omega_i^{+}+\omega_i^{-}=1$.
Note that $\Pi$ and $\Omega$ describe the initial and terminal state distributions of the random process itself, not those of any observed sequences.

The last parameter of interest, specified by the matrix $\Gamma=[\Gamma_{i,j}]_{i,j=1}^{S}$, defines the state transitions
\begin{eqnarray}
  \Gamma_{i,j} & = & p(S_{t+1}\!=\!\sigma_j\,|\,S_t\!=\!\sigma_i,\theta)\,.
\end{eqnarray}
Observe that the assumption of stationarity implies that $\Gamma$ is constant for all $t$.

Now, since the observed value sequence $\vec{x}_n$ is always here assumed to be known, we may for convenience define
\begin{eqnarray}
  o_{t,i} & = & p(X_t\!=\!x_t\,|\,S_t\!=\!\sigma_i,\theta)\,,
\end{eqnarray}
although we are not concerned here with the internal parameterisation structure of $o_{t,i}$ itself.
Thus, the explicitly parameterised version of model~\eqref{eq:1-stateful-markov-model} is given by
\begin{eqnarray}
  p(\iota_0,\vec{S}_n\!=\!\vec{s}_n,\vec{X}_n\!=\!\vec{x}_n,\tau_{n+1}\,|\,\theta) 
&\!\!=\!\!& 
  \pi_{i_1}\prod_{t=1}^{n-1}\Gamma_{i_t,i_{t+1}}\prod_{t=1}^{n}o_{t,i_t}\,\omega_{i_n}
\,,
\label{eq:param-markov-model}
\end{eqnarray}
where the unknown state sequence $\vec{s}_n$ corresponding to $\vec{x}_n$ is arbitrarily
specified by $\vec{s}_n=(\sigma_{i_1},\sigma_{i_2},\ldots,\sigma_{i_n})$.

Let us now suppose that we have observed an ordered set of value sequences
$\XX=\{(\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)})\}_{d=1}^{D}$.
Notionally, we may also define the correspondingly ordered set $\SS=\{\vec{s}^{(d)}\}_{d=1}^D$
of arbitrary state sequences.
Hence, under the assumption that the observed sequences are independent, the joint log-likelihood of the data is given by
\begin{eqnarray}
  L(\theta) & = & \log p(\SS,\XX\,|\,\theta) 
\nonumber\\& = & 
\log\prod_{d=1}^D 
p(\iota_0^{(d)},\vec{s}^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)}\,|\,\theta) 
\nonumber\\& = & 
\sum_{d=1}^D\log 
p(\iota_0^{(d)},\vec{s}^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)}\,|\,\theta) 
\nonumber\\& = & 
\sum_{d=1}^D L^{(d)}(\theta)
\,,
\end{eqnarray}
where
\begin{eqnarray}
  L^{(d)}(\theta) & = &
   \log\pi_{i_1^{(d)}}^{(d)}
 + \sum_{t=1}^{n^{(d)}-1}\log\Gamma_{i_t^{(d)},i_{t+1}^{(d)}}
 + \sum_{t=1}^{n^{(d)}}\log o_{t,i_t^{(d)}}
 + \log\omega_{i_{n^{(d)}}^{(d)}}^{(d)}
\,,
\end{eqnarray}
and $n^{(d)}=|\vec{x}^{(d)}|$.

However, recall that $\SS$ is actually uknown. Hence, we take an expectation of the log-likelihood over all possible values of
$\SS$, namely\footnote{Other expectations are possible, e.g.\ over the joint distribution $\SS,\XX\,|\,\theta$. This latter produces macro-averaged
parameter estimates of the form $\sum_{d=1}^D\phi^{(d)}/\sum_{d=1}^D\psi^{(d)}$, whereas the discriminative distribution $\SS\,|\,\XX,\theta$
often leads to micro-averaged estimates of the form $\sum_{d=1}^D\phi^{(d)}/\psi^{(d)}/D$.}
\begin{eqnarray}
  Q(\theta) & = & E_{\SS\,|\,\XX,\theta}\left[\log p(\SS,\XX\,|\,\theta)\right]
\nonumber\\& = & 
E_{\SS\,|\,\XX\theta}\left[
\sum_{d=1}^D L^{(d)}(\theta)
\right]
\nonumber\\& = & 
\sum_{d=1}^D E_{\SS\,|\,\XX,\theta}\left[
L^{(d)}(\theta)
\right]
\nonumber\\& = & 
\sum_{d=1}^D \sum_{i_1^{(d)}=1}^{S}\cdots\sum_{i^{(d)}_{n^{(d)}}}^{S}
p(\vec{s}^{(d)}\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\theta) 
\,L^{(d)}(\theta)
\,.
\end{eqnarray}
In practice, it is difficult to optimise this nonlinear expression analytically. A feasible alternative is to iteratively apply the {\em expectation--maximisation}
(EM) algorithm:
\begin{enumerate}
\item {\em Expectation step:} Compute the expected log-likelihood conditioned on a known parameter estimate $\hat{\theta}_k$,
namely
\begin{eqnarray}
  Q(\theta,\hat{\theta}_k) & = & E_{\SS\,|\,\XX,\hat{\theta}_k}\left[\log p(\SS,\XX\,|\,\theta)\right]
\nonumber\\& = &
\sum_{d=1}^D \sum_{i_1^{(d)}=1}^{S}\cdots\sum_{i^{(d)}_{n^{(d)}}}^{S} 
p(\vec{s}^{(d)}\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\hat{\theta}_k)
\,L^{(d)}(\theta)\,.
\end{eqnarray}

\item {\em Maximisation step:} Obtain the optimal parameter estimate $\hat{\theta}_{k+1}$ that maximises the
conditional expected log-likehood, namely
\begin{eqnarray}
\hat{\theta}_{k+1} & = & \arg\max_{\theta} Q(\theta,\hat{\theta}_k)\,.
\end{eqnarray}
\end{enumerate}
These two steps are iterated until $\hat{\theta}_k$ has converged to a value $\hat{\theta}^*$ that maximises 
$L(\hat{\theta}^*)=Q(\hat{\theta}^*,\hat{\theta}^*)$.

blah about additivity

\begin{eqnarray}
  \frac{\partial Q}{\partial\Gamma_{i,j}} & = & 
  \frac{\partial}{\partial\Gamma_{i,j}}
  \sum_{d=1}^D
  \sum_{i_1^{(d)}=1}^{S}\cdots\sum_{i^{(d)}_{n^{(d)}}}^{S} 
  p(\vec{s}^{(d)}\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\hat{\theta}')
  \sum_{t=1}^{n^{(d)}-1}\log\Gamma_{i_t^{(d)},i_{t+1}^{(d)}}
\nonumber\\& = &
  \sum_{d=1}^D
  \sum_{t=1}^{n^{(d)}-1}
  \sum_{i_1^{(d)}=1}^{S}\cdots\sum_{i^{(d)}_{n^{(d)}}}^{S} 
  \delta(i^{(d)}_t=i)\delta(i^{(d)}_{t+1}=j)
  \frac{p(\vec{s}^{(d)}\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\hat{\theta}')}
  {\Gamma_{i,j}}
\nonumber\\& = &
  \sum_{d=1}^D
  \sum_{t=1}^{n^{(d)}-1}
  \sum_{i_1^{(d)}=1}^{S}\cdots\sum_{i^{(d)}_{n^{(d)}}}^{S} 
  \delta(i^{(d)}_t=i)\delta(i^{(d)}_{t+1}=j)
  \frac{p(\vec{s}^{(d)}\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\hat{\theta}')}
  {\Gamma_{i,j}}
\nonumber\\& = &
  \frac{\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1}p(S_t=\sigma_i,S_{t+1}=\sigma_j\,|\,\iota_0^{(d)},\vec{x}^{(d)},\tau_{n^{(d)}+1}^{(d)},\hat{\theta}')}
  {\Gamma_{i,j}}
\nonumber\\& = &
  \frac{\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1} \xi_t^{(d)}(\sigma_i,\sigma_j;\hat{\theta}')}
  {\Gamma_{i,j}}
\end{eqnarray}
from equation~\eqref{eq:xi-def}. Now, subject to the constraint that $\sum_{j=1}^{S}\Gamma_{i,j}=1$, we induce the appropriate Lagrangian multiplier to
provide the proper normalisation, and hence derive that the optimal parameter estimate is given by
\begin{eqnarray}
  \hat{\Gamma}^*_{i,j} & = &
  \frac{\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1} \xi_t^{(d)}(\sigma_i,\sigma_j;\hat{\theta}')}
  {\sum_{j=1}^{S}\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1} \xi_t^{(d)}(\sigma_i,\sigma_j;\hat{\theta}')}
~=~
  \frac{\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1} \xi_t^{(d)}(\sigma_i,\sigma_j;\hat{\theta}')}
  {\sum_{d=1}^D\sum_{t=1}^{n^{(d)}-1} \gamma_t^{(d)}(\sigma_i;\hat{\theta}')}
\end{eqnarray}
from equation~\eqref{eq:gamma-def}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
