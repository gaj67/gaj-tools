\documentclass[a4paper]{article}
\usepackage{graphicx,subcaption}
\usepackage{amsmath,amsfonts}
\usepackage{qtree}
\title{Notes on Sequence Modelling}
\author{G.A. Jarrad}
\begin{document}
\maketitle
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\section{Introduction}\label{sec:intro}
blah, blah, blah

\section{Random Process Sequences}
\label{sec:random-processes}
Consider, in general terms, a random process $R$ that generates a sequence of variables,
$R_1,R_2,R_3,\ldots$, where the index $i$ gives the discrete {\em stage} in the sequence,
and each variable $R_i$ randomly takes some value $r_i\in{\cal R}$.
Then, for some arbitrary sequence length $n$, we define
\begin{eqnarray}
\vec{R}_n & = & (R_1,R_2,\ldots,R_n)
\end{eqnarray}
 to be a length-$n$ process sequence, i.e. $|\!\vec{R}_n\!|=n$,
and further define
\begin{eqnarray}
\vec{r}_n & = & (r_1,r_2,\ldots,r_n)\,\in\,{\cal R}^n
\end{eqnarray}
to be a corresponding length-$n$ observation sequence of values.
The probability (for a discrete-value process) or probability density (for a continuous-value process) 
of a given sequence $\vec{r}_n$ is then defined as
\begin{eqnarray}
p(\vec{R}_n=\vec{r}_n)
& = & p(R_1=r_1,\ldots,R_n=r_n)\,.
\end{eqnarray}
Hence, note that if $R$ is a discrete-value process then we must have
\begin{eqnarray}
\hspace*{-5mm}
\sum_{\vec{r}_n\in{\cal R}^n}
p(\vec{R}_n=\vec{r}_n)
& = & \sum_{r_1\in{\cal R}}\cdots\sum_{r_n\in{\cal R}}
p(R_1=r_1,\ldots,R_n=r_n)~~=~1\,.
\end{eqnarray}
Similarly, if $R$ is a continuous-value process, then we must instead have
\begin{eqnarray}
\hspace*{-5mm}
\int_{{\cal R}^n}
p(\vec{R}_n=\vec{r}_n)
\;d\!\vec{r}_n
& \!\!\!=\!\!\! & \int_{\cal R}\!\!\cdots\!\int_{\cal R}
p(R_1=r_1,\ldots,R_n=r_n)\;dr_1\cdots dr_n=1\,.
\end{eqnarray}
In other words, given the sequence length $n$, the set ${\cal R}^n$ of all possible sequences
$\vec{r}_n$ covers the entire probability space.

This summation property causes modelling problems if we do not know in advance the exact length of
a sequence. For example, if the set of all length-1 sequences already
covers the entire probability space, and so too does the set of all length-2 sequences,
then the set of all length-1 and length-2 sequences covers the space twice over.
This problem, however, only exists due to our ambiguous notion of a sequence.
In practice,  suppose we have observed a given sequence 
$\vec{r}_n$. How do we know if the
underlying process $R$ has terminated, or will instead
continue to generate another observed value
$r_{n+1}$, leading to the extended sequence
$\vec{r}_{n+1}$? 
Similarly, how do we know that the first observed value $r_1$ was not in fact
part of a longer, unobserved sequence of values $\ldots,r_{-2},r_{-1},r_0$?

In order to handle such difficulties, we distinguish between a so-called
{\em incomplete} sequence $\vec{R}_n$, and a {\em complete}
sequence $\langle\vec{R}_n\rangle$ that has definite stages of
initiation and termination. 
Thus, we see that each length-2 incomplete sequence $\vec{r}_2$ starts with a length-1 incomplete
sequence $\vec{r}_1$, so that measuring the set of all length-1 and length-2 incomplete sequences
amounts to double counting.

A complete sequence may be specified by introducing indicator variables
that define the start and end of the sequence.
Thus, indicator $\iota_0$ takes a value of 1 if the sequence
starts at stage 0 (i.e.\ just prior to $R_1$), 
or a value of 0 if it does not.
Similarly, indicator $\tau_{n+1}$ takes a value of 1 if the
sequence terminates at stage $n+1$ (i.e. immediately after $R_n$), or a value of 0 if it does not.
We may now also consider {\em partially complete} sequences, namely
the {\em start sequence}
$\langle\vec{R}_n$, which was initiated at stage 0 but not yet definitely terminated
(i.e.\ $\iota_0=1$ but $\tau_{n+1}$ is unknown),
and the {\em end sequence} $\vec{R}_n\rangle$, which was terminated at stage $n+1$ but not
definitely initiated at stage 0 (i.e. $\tau_{n+1}=1$ but $\iota_0$ is unknown).

The probability or probability density 
of a given complete sequence $\langle\vec{r}_n\rangle$ is now defined as
\begin{eqnarray}
p(\langle\vec{r}_n\rangle)
& = & p(\iota_0=1,R_1=r_1\ldots,R_n=r_n,\tau_{n+1}=1)\,;
\end{eqnarray}
likewise
\begin{eqnarray}
p(\langle\vec{r}_n)
& = & p(\iota_0=1,R_1=r_1\ldots,R_n=r_n)
\end{eqnarray}
for a start sequence, and
\begin{eqnarray}
p(\vec{r}_n\rangle)
& = & p(R_1=r_1\ldots,R_n=r_n,\tau_{n+1}=1)
\end{eqnarray}
for an end sequence.
In the special cases where we know in advance that the start sequence $\langle\vec{r}_n$ definitely does not terminate
at stage $n+1$ (i.e.\ $\tau_{n+1}=0$), and the end sequence $\vec{r}_n\rangle$ definitely does not initiate at stage
0 (i.e.\ $\iota_0=0$), we may write
\begin{eqnarray}
p(\langle\vec{r}_n!)
& = & p(\iota_0=1,R_1=r_1\ldots,R_n=r_n,\tau_{n+1}=0)\,,
\end{eqnarray}
and
\begin{eqnarray}
p(!\vec{r}_n\rangle)
& = & p(\iota_0=0,R_1=r_1\ldots,R_n=r_n,\tau_{n+1}=1)\,.
\end{eqnarray}

Hence, for a discrete-value process, the probability mass of all complete length-$n$ sequences is given by
\begin{eqnarray}
\hspace*{-5mm}
\sum_{\vec{r}_n\in{\cal R}^n}
p(\langle\vec{r}_n\rangle)
& = & p(\iota_0=1,\tau_{n+1}=1)
~\doteq~P(N=n)\,,
\end{eqnarray}
where we have introduced the random variable $N$ to denote the length of an
arbitrary complete sequence. 
%Similarly,
%\begin{eqnarray}
%\hspace*{-5mm}
%\sum_{\vec{r}_n\in{\cal R}^n}
%p(\langle\vec{r}_n!)
%& = & p(\iota_0=1,\tau_{n+1}=0)
%~\doteq~P(N>n)\,,
%\end{eqnarray}
For the corresponding continuous-value process, we likewise deduce that
\begin{eqnarray}
%\hspace*{-5mm}
\int_{{\cal R}^n}
p(\langle\vec{r}_n\rangle)\;d\!\vec{r}_n
& = & 
p(\iota_0=1,\tau_{n+1}=1)
~=~P(N=n)\,.
\end{eqnarray}
We therefore deduce that the set ${\cal R}^{*}=\bigcup_{n=0}^{\infty}{\cal R}^n$ 
of all complete sequences of arbitrary length
covers the entire probability space exactly once, since for the discrete-value case we have
\begin{eqnarray}
\sum_{\vec{r}_{*}\in{\cal R}^{*}}
p(\langle\vec{r}_{*}\rangle)
& = & \sum_{n=0}^{\infty}
\sum_{\vec{r}_n\in{\cal R}^n}
p(\langle\vec{r}_n\rangle)
~=~\sum_{n=0}^{\infty}P(N=n)~=~1\,,
\end{eqnarray}
and for the continuous-value case we have
\begin{eqnarray}
\int_{{\cal R}^{*}}
p(\langle\vec{r}_{*}\rangle)
\;d\!\vec{r}_{*}
& = & \sum_{n=0}^{\infty}
\int_{{\cal R}^n}
p(\langle\vec{r}_n\rangle)
\;d\!\vec{r}_n
~=~\sum_{n=0}^{\infty}P(N=n)~=~1\,.
\end{eqnarray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Process Sequences}
\label{sec:markov-processes}
In the previous section we defined random processes and the sequences they generate.
It is generally assumed that a random process $R$ obeys strict causality, such that the
values generated by the process form a {\em temporal} sequence, where the distribution of
values for variable $R_i$, at stage $i$, depends only upon the values generated previously
in the sequence at stages $i-1,i-2,\ldots,1$. In addition, for a complete sequence
the distribution of the variable $R_1$ at the initial stage depends strongly upon being first
in the sequence, and likewise the distribution of the variable $R_n$, for a length-$n$
sequence, depends strongly upon both the past values in the sequence and on the fact
that it is the final stage.

Causality therefore indicates that a probability model of the sequences generated by a random
process $R$ can be factored in terms of conditioning, at each stage $i$, the random
variable $R_i$ on the values of the previous variables $R_{i-1},R_{i-2},\ldots,R_1$.
Thus, for any given complete sequence $\langle\vec{r}_n\rangle$, we obtain
the model
\begin{eqnarray}
p(\langle\vec{r}_{n}\rangle)
& = & 
p(\iota_0=1)\;
p(R_1=r_1\;|\;\iota_0=1)\;
\nonumber\\&&
p(R_2=r_2\;|\;R_1=r_1,\iota_0=1)\cdots
\nonumber\\&&
p(R_n=r_n\;|\;R_{n-1}=r_{n-1},\ldots,R_1=r_1,\iota_0=1)
\nonumber\\&&
p(\tau_{n+1}=1\;|\;R_n=r_n,\ldots,R_1=r_1,\iota_0=1)
\\
& = &
p(\iota_0=1)\;
\prod_{i=1}^{n}
p(R_i=r_i\;|\;\vec{R}_{i-1}=\vec{r}_{i-1},\iota_0=1)
\nonumber\\&&
p(\tau_{n+1}=1\;|\;\vec{R}_{n}=\vec{r}_{n},\iota_0=1)
\,.
\label{eq:markov}
\end{eqnarray}
Recall from Section~\ref{sec:random-processes} that we may adjust this model to allow for partially complete or incomplete sequences
by suitably modifying the corresponding boundary conditions for $\iota_0$ and $\tau_{n+1}$.

In practice, this factored model is usually simplified further by limiting the 
conditional dependency on past values to a small number.
In particular, one might assume the (first-order) Markov property, such that the value at any stage
$i$ depends only upon the immediate past value at stage $i-1$, namely
\begin{eqnarray}
%\hspace*{-5mm}
p(\langle\vec{r}_{n}\rangle)
& = &
p(\iota_0=1)\;p(R_1=r_1\;|\;\iota_0=1)
\nonumber\\&&
\prod_{i=2}^{n}
p(R_i=r_i\;|\;R_{i-1}=r_{i-1})
\nonumber\\&&
p(\tau_{n+1}=1\;|\;R_{n}=r_{n})
\,.
\end{eqnarray}

Alternatively, equation~\eqref{eq:markov} may be generalised to an $m$-th order model
by limiting the dependency of $R_i$ to the previous $m$ values, namely
\begin{eqnarray}
p(R_i=r_i\;|\;\vec{R}_{i-1}=\vec{r}_{i-1},\iota_0=1)
\hspace*{-40mm} &&
\nonumber\\
& = &
\left\{
\begin{array}{ll}
p(R_i=r_i\;|\;\vec{R}_{i-m,i-1}=\vec{r}_{i-m,i-1})
& \mbox{if $i>m$}\,,
\\
p(R_i=r_i\;|\;\vec{R}_{i-1}=\vec{r}_{i-1}
,\iota_0=1)
& \mbox{if $i\le m$}\,.
\end{array}
\right.
\,,
\end{eqnarray}
where
\begin{eqnarray}
\vec{R}_{i,j}
& = &
(R_{i},R_{i+1},\ldots,R_{j})\,.
\end{eqnarray}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
