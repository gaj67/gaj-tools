\documentclass{report}
\usepackage{qtree}
\begin{document}

\chapter{Introduction}
blah, blah, blah

\chapter{Modelling Sequences}
\section{Random Processes}
\label{sec:random-processes}
Consider, in general terms, a random process $R$ that generates a sequence of variables,
$R_1,R_2,R_3,\ldots$, where the index $i$ gives the discrete {\em stage} in the sequence,
and each variable $R_i$ randomly takes a value $r_i\in{\cal R}$.
Then, for some arbitrary sequence length $n$, we define
\begin{eqnarray}
\stackrel{\rightarrow}{\bf R}_n & = & (R_1,R_2,\ldots,R_n)
\end{eqnarray}
 to be a length-$n$ sequence
of random variables, i.e. $|\!\stackrel{\rightarrow}{\bf R}_n\!|=n$,
and further define
\begin{eqnarray}
\stackrel{\rightarrow}{\bf r}_n & = & (r_1,r_2,\ldots,r_n)\,\in\,{\cal R}^n
\end{eqnarray}
to be a corresponding length-$n$ sequence of values.
The probability (for a discrete-value process) or probability density (for a continuous-value process) 
of a given sequence $\stackrel{\rightarrow}{\bf r}_n$ is then defined as
\begin{eqnarray}
P(\stackrel{\rightarrow}{\bf R}_n=\stackrel{\rightarrow}{\bf r}_n)
& = & P(R_1=r_1,\ldots,R_n=r_n)~=~p(r_1,\ldots,r_n)\,.
\end{eqnarray}
Hence, note that if $R$ is a discrete-value process then we must have
\begin{eqnarray}
\hspace*{-5mm}
\sum_{\stackrel{\rightarrow}{\bf r}_n\in{\cal R}^n}
P(\stackrel{\rightarrow}{\bf R}_n=\stackrel{\rightarrow}{\bf r}_n)
& = & \sum_{r_1\in{\cal R}}\cdots\sum_{r_n\in{\cal R}}
p(r_1,\ldots,r_n)~~=~1\,.
\end{eqnarray}
Alternatively, if $R$ is a continuous-value process, then we must similarly have
\begin{eqnarray}
\hspace*{-5mm}
\int_{{\cal R}^n}
P(\stackrel{\rightarrow}{\bf R}_n=\stackrel{\rightarrow}{\bf r}_n)
\;d\!\stackrel{\rightarrow}{\bf r}_n
& = & \int_{\cal R}\cdots\int_{\cal R}
p(r_1,\ldots,r_n)\;dr_1\cdots dr_n
~=~1\,.
\end{eqnarray}
In other words, given the sequence length $n$, the set ${\cal R}^n$ of all possible sequences
$\stackrel{\rightarrow}{\bf r}_n$ covers the entire probability space.

This latter property causes modelling problems if we do not know in advance the exact length of
a sequence; for example, the set of all length-1 and length-2 sequences already
covers twice the entire probability space.
In practice,  suppose we have observed a given sequence 
$\stackrel{\rightarrow}{\bf r}_n$. How do we know if the
underlying process $R$ has terminated, or will instead
continue to generate another observed value
$r_{n+1}$, leading to the extended sequence
$\stackrel{\rightarrow}{\bf r}_{n+1}$? 
Similarly, how do we know that the first observed value $r_1$ was not in fact
part of a longer, unobserved sequence of values $\ldots,r_{-2},r_{-1},r_0$?

In order to handle such difficulties, we distinguish between a so-called
{\em incomplete} sequence $\stackrel{\rightarrow}{\bf r}_n$, and a {\em complete}
sequence $<\stackrel{\rightarrow}{\bf r}_n>$ that has definite stages of
initiation and termination. 
Thus, we see that each length-2 incomplete sequence starts with a length-1 incomplete
sequence, so that measuring the set of all length-1 and length-2 incomplete sequences
amounts to double counting.

A complete sequence may be specified by introducing indicator variables
that define the start and end of the sequence.
Thus, indicator $I_0$ that takes a value of 1 if the sequence
starts at stage 0 (i.e. just prior to value $r_1$ in the complete sequence), 
or a value of 0 if it does not.
Similarly, indicator $T_{n+1}$ that takes a value of 1 if the
sequence terminates at stage $n+1$ (i.e. just after value $r_n$ in the complete sequence), or a value of 0 if it does not.
As one consequence, we may now also consider the two {\em partially complete} sequences, namely
$<\stackrel{\rightarrow}{\bf r}_n$, which was initiated at stage 0 but not yet terminated
(i.e. $I_0=1$ but $T_{n+1}=0$),
and $\stackrel{\rightarrow}{\bf r}_n>$, which was terminated at stage $n+1$ but not
initiated at stage 0 (i.e. $T_{n+1}=1$ but $I_0=0$).
As another consequence, these definitions of the indicator variables permit empty, or
length-0, sequences. If this is not desirable, then we could replace indicators
$I_0$ and $T_{n+1}$ by new indicators $I_{1}'$ and $T_{n}'$, respectively.
Then $I_{1}'$ takes the value 1 (or 0) if $r_1$ is (or is not) the first observation in a
complete sequence, and $T_{n}'$ takes the value 1 (or 0) if $r_n$ is
(or is not) the final observation in the complete sequence.

The probability or probability density 
of a given complete sequence $<\stackrel{\rightarrow}{\bf r}_n>$ is now defined as
\begin{eqnarray}
P(<\stackrel{\rightarrow}{\bf r}_n>)
& = & P(I_0=1,R_1=r_1\ldots,R_n=r_n,T_{n+1}=1)\,.
\end{eqnarray}
Hence, for a discrete-value process we now obtain
\begin{eqnarray}
\hspace*{-5mm}
\sum_{\stackrel{\rightarrow}{\bf r}_n\in{\cal R}^n}
P(<\stackrel{\rightarrow}{\bf r}_n>)
& = & P(I_0=1,T_{n+1}=1)
~\doteq~P(N=n)\,,
\end{eqnarray}
where we have introduced the random variable $N$ to denote the length of an
arbitrary complete sequence. 
For the corresponding continuous-value process, we likewise deduce that
\begin{eqnarray}
%\hspace*{-5mm}
\int_{{\cal R}^n}
P(<\stackrel{\rightarrow}{\bf r}_n>)\;d\!\stackrel{\rightarrow}{\bf r}_n
& = & 
P(I_0=1,T_{n+1}=1)
~\doteq~P(N=n)\,.
\end{eqnarray}
We therefore deduce that the set ${\cal R}^{*}=\bigcup_{n=0}^{\infty}{\cal R}^n$ 
of all complete sequences of arbitrary length
covers the entire probability space exactly once, since for the discrete-value case we have
\begin{eqnarray}
\sum_{<\stackrel{\rightarrow}{\bf r}_{*}>\in{\cal R}^{*}}
P(<\stackrel{\rightarrow}{\bf r}_{*}>)
& = & \sum_{n=0}^{\infty}
\sum_{\stackrel{\rightarrow}{\bf r}_n\in{\cal R}^n}
P(<\stackrel{\rightarrow}{\bf r}_n>)
\nonumber\\
& = & \sum_{n=0}^{\infty}P(N=n)~=~1\,,
\end{eqnarray}
and for the continuous-value case we have
\begin{eqnarray}
\int_{{\cal R}^{*}}
P(<\stackrel{\rightarrow}{\bf r}_{*}>)
\;d\!\stackrel{\rightarrow}{\bf r}_{*}
& = & \sum_{n=0}^{\infty}
\int_{{\cal R}^n}
P(<\stackrel{\rightarrow}{\bf r}_n>)
\;d\!\stackrel{\rightarrow}{\bf r}_n
\nonumber\\
& = & \sum_{n=0}^{\infty}P(N=n)~=~1\,.
\end{eqnarray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Processes}
In the previous section we defined random processes and the sequences they generate.
It is generally assumed that a random process $R$ obeys strict causality, such that the
values generated by the process form a {\em temporal} sequence, where the distribution of
values for variable $R_i$, at stage $i$, depends only upon the values generated previously
in the sequence at stages $i-1,i-2,\ldots,1$. In addition, for a complete sequence
the distribution of the variable $R_1$ at the initial stage depends strongly upon being first
in the sequence, and likewise the distribution of the variable $R_n$, for a length-$n$
sequence, depends strongly upon both the past values in the sequence and on the fact
that it is the final stage.

Causality therefore indicates that a probability model of the sequences generated by a random
process $R$ can be factored in terms of conditioning, at each stage $i$, the random
variable $R_i$ on the values of the previous variables $R_{i-1},R_{i-2},\ldots,R_1$.
Thus, for any given complete sequence $<\stackrel{\rightarrow}{\bf r}_n>$, we obtain
the model
\begin{eqnarray}
P(<\stackrel{\rightarrow}{\bf r}_{n}>)
& = & 
P(I_0=1)\;
P(R_1=r_1\;|\;I_0=1)\;
\nonumber\\&&
P(R_2=r_2\;|\;R_1=r_1,I_0=1)\cdots
\nonumber\\&&
P(R_n=r_n\;|\;R_{n-1}=r_{n-1},\ldots,R_1=r_1,I_0=1)
\nonumber\\&&
P(T_{n+1}=1\;|\;R_n=r_n,\ldots,R_1=r_1,I_0=1)
\\
& = &
P(I_0=1)\;
\prod_{i=1}^{n}
P(R_i=r_i\;|\;\stackrel{\rightarrow}{\bf R}_{i-1}=\stackrel{\rightarrow}{\bf r}_{i-1},I_0=1)
\nonumber\\&&
P(T_{n+1}=1\;|\;\stackrel{\rightarrow}{\bf R}_{n}=\stackrel{\rightarrow}{\bf r}_{n},I_0=1)
\,.
\end{eqnarray}
Observe that we may adjust this model to allow for partially complete or incomplete sequences
by modifying the corresponding boundary conditions $I_0$ and $T_{n+1}$.

In practice, this factored model is usually simplified further by limiting the 
conditional dependency on past values to a small number.
In particular, one might assume the Markov property, such that the value at any stage
$i$ depends only upon the immediate past value at stage $i-1$, namely
\begin{eqnarray}
%\hspace*{-5mm}
P(<\stackrel{\rightarrow}{\bf r}_{n}>)
& = &
P(I_0=1)\;P(R_1=r_1\;|\;I_0=1)
\nonumber\\&&
\prod_{i=2}^{n}
P(R_i=r_i\;|\;R_{i-1}=r_{i-1})
\nonumber\\&&
P(T_{n+1}=1\;|\;R_{n}=r_{n})
\,.
\end{eqnarray}
Alternatively, this 1st order Markov model may be generalised to an $m$-th order model
by limiting the dependency of $R_i$ to only the previous $m$ values, namely
\begin{eqnarray}
P(R_i=r_i\;|\;\stackrel{\rightarrow}{\bf R}_{i-1}=\stackrel{\rightarrow}{\bf r}_{i-1},I_0=1)
\hspace*{-40mm} &&
\nonumber\\
& \doteq &
\left\{
\begin{array}{ll}
P(R_i=r_i\;|\;\stackrel{\rightarrow}{\bf R}_{i-m,i-1}=\stackrel{\rightarrow}{\bf r}_{i-m,i-1})
& \mbox{if $i\ge m+1$}\,,
\\
P(R_i=r_i\;|\;\stackrel{\rightarrow}{\bf R}_{i-1}=\stackrel{\rightarrow}{\bf r}_{i-1}
,I_0=1)
& \mbox{if $i\le m$}\,.
\end{array}
\right.
\,.
\end{eqnarray}
where
\begin{eqnarray}
\stackrel{\rightarrow}{\bf R}_{i,j}
& = &
(R_{i},R_{i+1},\ldots,R_{j})\,.
\end{eqnarray}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
